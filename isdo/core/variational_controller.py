"""
Variational Controller Implementation for ISDO
=============================================

å¯¦ç¾è®Šåˆ†æœ€å„ªæŽ§åˆ¶å•é¡Œçš„æ ¸å¿ƒç®—æ³•ï¼Œå°‡æ“´æ•£æŽ¡æ¨£è½‰åŒ–ç‚ºæœ€å„ªæŽ§åˆ¶å•é¡Œã€‚
åŒ…æ‹¬å‹•ä½œç©åˆ†è¨ˆç®—ã€Euler-Lagrange æ–¹ç¨‹æ±‚è§£å’Œ Hamilton-Jacobi-Bellman æ–¹ç¨‹ã€‚

æ•¸å­¸èƒŒæ™¯:
- è®Šåˆ†æœ€å„ªæŽ§åˆ¶: min âˆ« L(x, áº‹, Ïƒ) dÏƒ
- Euler-Lagrange æ–¹ç¨‹: d/dÏƒ(âˆ‚L/âˆ‚áº‹) - âˆ‚L/âˆ‚x = 0
- Hamilton-Jacobi-Bellman: âˆ‚V/âˆ‚Ïƒ + H(x, âˆ‡V, Ïƒ) = 0
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional, Union, Dict, List, Callable
import math

# å»¶é²å°Žå…¥é¿å…å¾ªç’°ä¾è³´
def _get_variational_ode_system():
    from .variational_ode_system import VariationalODESystem
    return VariationalODESystem

def _get_hilbert_space():
    from .hilbert_space import HilbertSpace
    return HilbertSpace


class ActionIntegral:
    """
    å‹•ä½œç©åˆ†è¨ˆç®—å™¨

    å¯¦ç¾è®Šåˆ†æœ€å„ªæŽ§åˆ¶ä¸­çš„å‹•ä½œæ³›å‡½:
    ð’œ[x] = âˆ«[Ïƒ_max to 0] â„’(x, áº‹, Ïƒ) dÏƒ
    """

    def __init__(
        self,
        regularization_lambda: float = 0.01,
        curvature_penalty: float = 0.001,
        domain_size: Tuple[float, ...] = (1.0, 1.0)
    ):
        """
        åˆå§‹åŒ–å‹•ä½œç©åˆ†

        Args:
            regularization_lambda: æ­£å‰‡åŒ–åƒæ•¸ Î»
            curvature_penalty: æ›²çŽ‡æ‡²ç½°ä¿‚æ•¸
            domain_size: å®šç¾©åŸŸå¤§å°
        """
        self.lambda_reg = regularization_lambda
        self.curvature_penalty = curvature_penalty
        self.domain_size = domain_size

    def compute_lagrangian(
        self,
        x: torch.Tensor,
        x_dot: torch.Tensor,
        f_denoiser: torch.Tensor,
        sigma: float,
        grad_f: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è¨ˆç®—æ‹‰æ ¼æœ—æ—¥å‡½æ•¸ â„’(x, áº‹, Ïƒ)

        â„’ = Â½|áº‹ - f(x;Ïƒ)|Â²_H + Î»|âˆ‡_x f|Â²_op + Î¼|âˆ‡Â²x|Â²

        Args:
            x: ç•¶å‰ç‹€æ…‹
            x_dot: ç‹€æ…‹å°Žæ•¸ dx/dÏƒ
            f_denoiser: åŽ»å™ªå‡½æ•¸è¼¸å‡º f(x;Ïƒ)
            sigma: å™ªè²æ°´å¹³
            grad_f: f çš„æ¢¯åº¦ (å¯é¸)

        Returns:
            lagrangian: æ‹‰æ ¼æœ—æ—¥å‡½æ•¸å€¼
        """
        batch_size = x.shape[0] if x.dim() > 2 else 1

        # ä¸»è¦é …: Â½|áº‹ - f|Â²_H
        drift_term = f_denoiser / sigma if sigma > 1e-8 else f_denoiser
        velocity_error = x_dot - drift_term

        # LÂ² ç¯„æ•¸çš„ä¸»è¦è²¢ç»
        main_term = 0.5 * torch.sum(velocity_error ** 2, dim=(-2, -1))

        # æ­£å‰‡åŒ–é …: Î»|âˆ‡_x f|Â²
        regularization_term = torch.tensor(0.0, device=x.device, dtype=x.dtype)
        if grad_f is not None:
            grad_norm_sq = torch.sum(grad_f ** 2, dim=(-2, -1))
            regularization_term = self.lambda_reg * grad_norm_sq
        else:
            # æ•¸å€¼è¿‘ä¼¼æ¢¯åº¦
            grad_f_approx = self._approximate_gradient(f_denoiser)
            grad_norm_sq = torch.sum(grad_f_approx ** 2, dim=(-2, -1))
            regularization_term = self.lambda_reg * grad_norm_sq

        # æ›²çŽ‡æ‡²ç½°é …: Î¼|âˆ‡Â²x|Â²
        curvature_term = self._compute_curvature_penalty(x)

        lagrangian = main_term + regularization_term + self.curvature_penalty * curvature_term

        # ç¢ºä¿å½¢ç‹€ä¸€è‡´æ€§
        if lagrangian.dim() == 0:
            lagrangian = lagrangian.unsqueeze(0)
        if batch_size > 1 and lagrangian.shape[0] != batch_size:
            lagrangian = lagrangian.expand(batch_size)

        return lagrangian

    def _approximate_gradient(self, f: torch.Tensor) -> torch.Tensor:
        """
        æ•¸å€¼è¿‘ä¼¼æ¢¯åº¦è¨ˆç®—
        """
        # ä½¿ç”¨ Sobel ç®—å­è¿‘ä¼¼æ¢¯åº¦
        if f.dim() == 4:  # (B, C, H, W)
            # x æ–¹å‘æ¢¯åº¦
            grad_x = F.conv2d(
                f,
                torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]],
                           device=f.device, dtype=f.dtype).expand(f.shape[1], 1, -1, -1),
                padding=1,
                groups=f.shape[1]
            ) / 8.0

            # y æ–¹å‘æ¢¯åº¦
            grad_y = F.conv2d(
                f,
                torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]],
                           device=f.device, dtype=f.dtype).expand(f.shape[1], 1, -1, -1),
                padding=1,
                groups=f.shape[1]
            ) / 8.0

            # æ¢¯åº¦æ¨¡é•·
            grad_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-12)

        elif f.dim() == 2:  # (H, W)
            # ç°¡å–®å·®åˆ†
            grad_x = torch.zeros_like(f)
            grad_y = torch.zeros_like(f)

            grad_x[1:-1, :] = (f[2:, :] - f[:-2, :]) / 2.0
            grad_y[:, 1:-1] = (f[:, 2:] - f[:, :-2]) / 2.0

            grad_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-12)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å¼µé‡ç¶­åº¦: {f.shape}")

        return grad_magnitude

    def _compute_curvature_penalty(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—æ›²çŽ‡æ‡²ç½°é … |âˆ‡Â²x|Â²
        """
        if x.dim() == 4:  # (B, C, H, W)
            # äºŒéšŽå°Žæ•¸ (æ‹‰æ™®æ‹‰æ–¯ç®—å­)
            laplacian = F.conv2d(
                x,
                torch.tensor([[[[0, 1, 0], [1, -4, 1], [0, 1, 0]]]],
                           device=x.device, dtype=x.dtype).expand(x.shape[1], 1, -1, -1),
                padding=1,
                groups=x.shape[1]
            )

        elif x.dim() == 2:  # (H, W)
            # æ‰‹å‹•è¨ˆç®—æ‹‰æ™®æ‹‰æ–¯ç®—å­
            laplacian = torch.zeros_like(x)
            laplacian[1:-1, 1:-1] = (
                x[2:, 1:-1] + x[:-2, 1:-1] + x[1:-1, 2:] + x[1:-1, :-2] - 4 * x[1:-1, 1:-1]
            )
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å¼µé‡ç¶­åº¦: {x.shape}")

        curvature_norm_sq = torch.sum(laplacian ** 2, dim=(-2, -1))
        return curvature_norm_sq

    def compute_action(
        self,
        trajectory: torch.Tensor,
        sigma_schedule: torch.Tensor,
        denoiser_function: Callable,
        extra_args: Optional[Dict] = None
    ) -> torch.Tensor:
        """
        è¨ˆç®—æ•´å€‹è»Œè·¡çš„å‹•ä½œç©åˆ†

        ð’œ[x] = âˆ«[Ïƒ_max to 0] â„’(x, áº‹, Ïƒ) dÏƒ

        Args:
            trajectory: è»Œè·¡ (T, B, C, H, W)
            sigma_schedule: å™ªè²æ°´å¹³åºåˆ— (T,)
            denoiser_function: åŽ»å™ªå‡½æ•¸
            extra_args: é¡å¤–åƒæ•¸

        Returns:
            action: å‹•ä½œå€¼
        """
        extra_args = extra_args or {}
        T = len(sigma_schedule)

        # è¨ˆç®—è»Œè·¡å°Žæ•¸
        x_dot = self._compute_trajectory_derivative(trajectory, sigma_schedule)

        action = torch.tensor(0.0, device=trajectory.device, dtype=trajectory.dtype)

        for t in range(T - 1):
            x_t = trajectory[t]
            x_dot_t = x_dot[t]
            sigma_t = sigma_schedule[t]

            # ç²å–åŽ»å™ªå‡½æ•¸è¼¸å‡º
            s_in = torch.ones(x_t.shape[0], device=x_t.device) * sigma_t
            f_t = denoiser_function(x_t, s_in, **extra_args)

            # è¨ˆç®—æ‹‰æ ¼æœ—æ—¥å‡½æ•¸
            lagrangian_t = self.compute_lagrangian(x_t, x_dot_t, f_t, sigma_t.item())

            # ç©åˆ† (æ¢¯å½¢æ³•å‰‡)
            dt = (sigma_schedule[t] - sigma_schedule[t + 1]).abs()
            action = action + lagrangian_t * dt

        return action

    def _compute_trajectory_derivative(
        self,
        trajectory: torch.Tensor,
        sigma_schedule: torch.Tensor
    ) -> torch.Tensor:
        """
        è¨ˆç®—è»Œè·¡çš„æ™‚é–“å°Žæ•¸ dx/dÏƒ
        """
        T = trajectory.shape[0]
        x_dot = torch.zeros_like(trajectory)

        # ä¸­å¿ƒå·®åˆ†
        for t in range(1, T - 1):
            dt_forward = (sigma_schedule[t] - sigma_schedule[t + 1]).abs()
            dt_backward = (sigma_schedule[t - 1] - sigma_schedule[t]).abs()

            if dt_forward > 1e-12 and dt_backward > 1e-12:
                # è‡ªé©æ‡‰å·®åˆ†æ¬Šé‡
                w_f = dt_backward / (dt_forward + dt_backward)
                w_b = dt_forward / (dt_forward + dt_backward)

                x_dot[t] = w_f * (trajectory[t + 1] - trajectory[t]) / dt_forward + \
                          w_b * (trajectory[t] - trajectory[t - 1]) / dt_backward

        # é‚Šç•Œæ¢ä»¶
        if T > 1:
            dt_0 = (sigma_schedule[0] - sigma_schedule[1]).abs()
            if dt_0 > 1e-12:
                x_dot[0] = (trajectory[1] - trajectory[0]) / dt_0

            dt_T = (sigma_schedule[T-2] - sigma_schedule[T-1]).abs()
            if dt_T > 1e-12:
                x_dot[T-1] = (trajectory[T-1] - trajectory[T-2]) / dt_T

        return x_dot


class EulerLagrangeSystem:
    """
    Euler-Lagrange æ–¹ç¨‹ç³»çµ±æ±‚è§£å™¨

    æ±‚è§£è®Šåˆ†å•é¡Œçš„å¿…è¦æ¢ä»¶:
    d/dÏƒ(âˆ‚â„’/âˆ‚áº‹) - âˆ‚â„’/âˆ‚x = 0
    """

    def __init__(
        self,
        action_integral: ActionIntegral,
        numerical_epsilon: float = 1e-6
    ):
        """
        åˆå§‹åŒ– Euler-Lagrange ç³»çµ±

        Args:
            action_integral: å‹•ä½œç©åˆ†è¨ˆç®—å™¨
            numerical_epsilon: æ•¸å€¼å¾®åˆ†ç²¾åº¦
        """
        self.action_integral = action_integral
        self.eps = numerical_epsilon

    def compute_euler_lagrange_residual(
        self,
        x: torch.Tensor,
        x_dot: torch.Tensor,
        x_ddot: torch.Tensor,
        f_denoiser: torch.Tensor,
        sigma: float,
        grad_f: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è¨ˆç®— Euler-Lagrange æ–¹ç¨‹çš„æ®˜å·®

        Args:
            x: ç•¶å‰ç‹€æ…‹
            x_dot: ä¸€éšŽå°Žæ•¸
            x_ddot: äºŒéšŽå°Žæ•¸
            f_denoiser: åŽ»å™ªå‡½æ•¸
            sigma: å™ªè²æ°´å¹³
            grad_f: f çš„æ¢¯åº¦

        Returns:
            residual: EL æ–¹ç¨‹æ®˜å·®
        """
        # è¨ˆç®— âˆ‚â„’/âˆ‚áº‹
        partial_L_partial_xdot = self._compute_partial_L_partial_xdot(
            x, x_dot, f_denoiser, sigma
        )

        # è¨ˆç®— d/dÏƒ(âˆ‚â„’/âˆ‚áº‹) (è¿‘ä¼¼ç‚ºæ™‚é–“å°Žæ•¸)
        d_partial_L_partial_xdot = x_ddot - f_denoiser / (sigma + 1e-8)

        # è¨ˆç®— âˆ‚â„’/âˆ‚x
        partial_L_partial_x = self._compute_partial_L_partial_x(
            x, x_dot, f_denoiser, sigma, grad_f
        )

        # Euler-Lagrange æ–¹ç¨‹: d/dÏƒ(âˆ‚â„’/âˆ‚áº‹) - âˆ‚â„’/âˆ‚x = 0
        residual = d_partial_L_partial_xdot - partial_L_partial_x

        return residual

    def _compute_partial_L_partial_xdot(
        self,
        x: torch.Tensor,
        x_dot: torch.Tensor,
        f_denoiser: torch.Tensor,
        sigma: float
    ) -> torch.Tensor:
        """
        è¨ˆç®— âˆ‚â„’/âˆ‚áº‹

        å°æ–¼ â„’ = Â½|áº‹ - f|Â², æœ‰ âˆ‚â„’/âˆ‚áº‹ = áº‹ - f
        """
        drift_term = f_denoiser / (sigma + 1e-8)
        return x_dot - drift_term

    def _compute_partial_L_partial_x(
        self,
        x: torch.Tensor,
        x_dot: torch.Tensor,
        f_denoiser: torch.Tensor,
        sigma: float,
        grad_f: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è¨ˆç®— âˆ‚â„’/âˆ‚x (ä½¿ç”¨æ•¸å€¼å¾®åˆ†)
        """
        # ç”±æ–¼ â„’ å° x çš„ä¾è³´ä¸»è¦é€šéŽ f(x;Ïƒ)ï¼Œéœ€è¦è¨ˆç®— âˆ‚f/âˆ‚x
        # é€™éœ€è¦å°åŽ»å™ªæ¨¡åž‹é€²è¡Œæ•¸å€¼å¾®åˆ†

        if grad_f is not None:
            # å¦‚æžœæä¾›äº†è§£æžæ¢¯åº¦
            partial_f_partial_x = grad_f
        else:
            # æ•¸å€¼å¾®åˆ†è¿‘ä¼¼
            partial_f_partial_x = self._numerical_gradient_f(x, f_denoiser)

        # âˆ‚â„’/âˆ‚x â‰ˆ -(áº‹ - f/Ïƒ) Â· âˆ‚f/âˆ‚x / Ïƒ + æ­£å‰‡åŒ–é …
        drift_term = f_denoiser / (sigma + 1e-8)
        velocity_error = x_dot - drift_term

        # ä¸»è¦é …
        main_term = -torch.sum(
            velocity_error.unsqueeze(-1).unsqueeze(-1) * partial_f_partial_x,
            dim=1, keepdim=True
        ) / (sigma + 1e-8)

        # æ­£å‰‡åŒ–é …çš„æ¢¯åº¦
        reg_term = self.action_integral.lambda_reg * self._compute_regularization_gradient(x)

        # æ›²çŽ‡æ‡²ç½°é …çš„æ¢¯åº¦
        curvature_grad = self.action_integral.curvature_penalty * self._compute_curvature_gradient(x)

        return main_term + reg_term + curvature_grad

    def _numerical_gradient_f(self, x: torch.Tensor, f: torch.Tensor) -> torch.Tensor:
        """
        æ•¸å€¼è¨ˆç®— âˆ‚f/âˆ‚x
        """
        # ç°¡åŒ–å¯¦ç¾ï¼šä½¿ç”¨æœ‰é™å·®åˆ†
        grad_f = torch.zeros_like(x)

        if x.dim() == 4:  # (B, C, H, W)
            # x æ–¹å‘
            grad_f[:, :, 1:-1, :] = (f[:, :, 2:, :] - f[:, :, :-2, :]) / (2 * self.eps)
            # y æ–¹å‘ (ç°¡åŒ–ç‚ºåªè¨ˆç®— x æ–¹å‘)
            grad_f[:, :, :, 1:-1] += (f[:, :, :, 2:] - f[:, :, :, :-2]) / (2 * self.eps)

        return grad_f

    def _compute_regularization_gradient(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—æ­£å‰‡åŒ–é …çš„æ¢¯åº¦
        """
        # ç°¡åŒ–å¯¦ç¾
        return torch.zeros_like(x)

    def _compute_curvature_gradient(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—æ›²çŽ‡æ‡²ç½°é …çš„æ¢¯åº¦
        """
        # âˆ‡(|âˆ‡Â²x|Â²) = 2âˆ‡Â²(âˆ‡Â²x) = 2âˆ‡â´x
        # ç°¡åŒ–ç‚ºæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„æ‹‰æ™®æ‹‰æ–¯

        if x.dim() == 4:  # (B, C, H, W)
            # å››éšŽå¾®åˆ†ç®—å­ (ç°¡åŒ–å¯¦ç¾)
            biharmonic = F.conv2d(
                x,
                torch.tensor([[[[0, 0, 1, 0, 0],
                              [0, 2, -8, 2, 0],
                              [1, -8, 20, -8, 1],
                              [0, 2, -8, 2, 0],
                              [0, 0, 1, 0, 0]]]],
                           device=x.device, dtype=x.dtype).expand(x.shape[1], 1, -1, -1),
                padding=2,
                groups=x.shape[1]
            )
        else:
            biharmonic = torch.zeros_like(x)

        return 2 * biharmonic


class VariationalController:
    """
    è®Šåˆ†æœ€å„ªæŽ§åˆ¶å™¨

    ISDO ç®—æ³•çš„æ ¸å¿ƒæŽ§åˆ¶å™¨ï¼Œå¯¦ç¾å¾žå™ªè²åˆ°æ•¸æ“šçš„æœ€å„ªè·¯å¾‘æ±‚è§£ã€‚
    """

    def __init__(
        self,
        spatial_dims: Tuple[int, ...],
        regularization_lambda: float = 0.01,
        curvature_penalty: float = 0.001,
        sobolev_order: float = 1.0,
        device: Optional[torch.device] = None
    ):
        """
        åˆå§‹åŒ–è®Šåˆ†æŽ§åˆ¶å™¨

        Args:
            spatial_dims: ç©ºé–“ç¶­åº¦
            regularization_lambda: æ­£å‰‡åŒ–åƒæ•¸
            curvature_penalty: æ›²çŽ‡æ‡²ç½°
            sobolev_order: Sobolev ç©ºé–“éšŽæ•¸
            device: è¨ˆç®—è¨­å‚™
        """
        self.spatial_dims = spatial_dims
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆå§‹åŒ–çµ„ä»¶
        self.action_integral = ActionIntegral(
            regularization_lambda=regularization_lambda,
            curvature_penalty=curvature_penalty,
            domain_size=(1.0, 1.0)
        )

        self.euler_lagrange_system = EulerLagrangeSystem(self.action_integral)

        self.hilbert_space = _get_hilbert_space()(
            spatial_dims=spatial_dims,
            sobolev_order=sobolev_order,
            device=device
        )

    def compute_optimal_control(
        self,
        x_current: torch.Tensor,
        sigma_current: float,
        denoiser_function: Callable,
        target_sigma: float,
        num_steps: int = 10,
        extra_args: Optional[Dict] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è¨ˆç®—æœ€å„ªæŽ§åˆ¶è»Œè·¡

        æ±‚è§£è®Šåˆ†å•é¡Œ: min âˆ« â„’(x, áº‹, Ïƒ) dÏƒ

        Args:
            x_current: ç•¶å‰ç‹€æ…‹
            sigma_current: ç•¶å‰å™ªè²æ°´å¹³
            denoiser_function: åŽ»å™ªå‡½æ•¸
            target_sigma: ç›®æ¨™å™ªè²æ°´å¹³
            num_steps: ç©åˆ†æ­¥æ•¸
            extra_args: é¡å¤–åƒæ•¸

        Returns:
            optimal_trajectory: æœ€å„ªè»Œè·¡
            optimal_controls: æœ€å„ªæŽ§åˆ¶åºåˆ—
        """
        extra_args = extra_args or {}

        # ç”Ÿæˆ Ïƒ èª¿åº¦
        sigma_schedule = torch.linspace(
            sigma_current, target_sigma, num_steps + 1, device=self.device
        )

        # åˆå§‹åŒ–è»Œè·¡
        trajectory = torch.zeros(
            num_steps + 1, *x_current.shape, device=self.device, dtype=x_current.dtype
        )
        trajectory[0] = x_current

        controls = torch.zeros_like(trajectory[:-1])

        # ä½¿ç”¨è®Šåˆ†åŽŸç†æ±‚è§£æœ€å„ªè»Œè·¡
        for t in range(num_steps):
            x_t = trajectory[t]
            sigma_t = sigma_schedule[t]
            sigma_next = sigma_schedule[t + 1]

            # è¨ˆç®—æœ€å„ªæŽ§åˆ¶
            u_optimal = self._solve_optimal_control_step(
                x_t, sigma_t, sigma_next, denoiser_function, extra_args
            )

            controls[t] = u_optimal

            # æ›´æ–°ç‹€æ…‹
            dt = sigma_next - sigma_t
            trajectory[t + 1] = x_t + u_optimal * dt

        return trajectory, controls

    def compute_optimal_control_with_ode_system(
        self,
        x_current: torch.Tensor,
        sigma_current: float,
        denoiser_function: Callable,
        target_sigma: float,
        spectral_order: int = 256,
        extra_args: Optional[Dict] = None,
        use_adaptive_stepping: bool = True
    ) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        """
        ä½¿ç”¨ VariationalODESystem è¨ˆç®—é«˜ç²¾åº¦æœ€å„ªæŽ§åˆ¶è»Œè·¡

        é€™æ˜¯æ”¹é€²çš„æ±‚è§£æ–¹æ³•ï¼Œä½¿ç”¨å®Œæ•´çš„è®Šåˆ† ODE ç³»çµ±

        Args:
            x_current: ç•¶å‰ç‹€æ…‹
            sigma_current: ç•¶å‰å™ªè²æ°´å¹³
            denoiser_function: åŽ»å™ªå‡½æ•¸
            target_sigma: ç›®æ¨™å™ªè²æ°´å¹³
            spectral_order: è­œæˆªæ–·éšŽæ•¸
            extra_args: é¡å¤–åƒæ•¸
            use_adaptive_stepping: æ˜¯å¦ä½¿ç”¨è‡ªé©æ‡‰æ­¥é•·

        Returns:
            optimal_trajectory: æœ€å„ªè»Œè·¡
            trajectory_info: è»Œè·¡è©³ç´°ä¿¡æ¯
            solve_info: æ±‚è§£çµ±è¨ˆä¿¡æ¯
        """
        from ..samplers.unified_model_wrapper import UnifiedModelWrapper

        extra_args = extra_args or {}

        # å‰µå»º VariationalODESystem å¯¦ä¾‹
        VariationalODESystem = _get_variational_ode_system()
        ode_system = VariationalODESystem(
            spatial_dims=x_current.shape[-2:],
            spectral_order=spectral_order,
            sobolev_order=self.hilbert_space.sobolev_order,
            regularization_lambda=self.action_integral.lambda_reg,
            sobolev_penalty=self.action_integral.curvature_penalty,
            device=self.device
        )

        # åŒ…è£åŽ»å™ªå‡½æ•¸
        if not isinstance(denoiser_function, UnifiedModelWrapper):
            model_wrapper = UnifiedModelWrapper(denoiser_function, device=self.device)
        else:
            model_wrapper = denoiser_function

        # æŠ•å½±åˆ°è­œç©ºé–“
        initial_coefficients = ode_system.spectral_basis.project_to_basis(x_current)

        # ä½¿ç”¨ VariationalODESystem æ±‚è§£
        final_coefficients, solution_trajectory, solve_info = ode_system.solve_variational_ode(
            initial_coefficients=initial_coefficients,
            sigma_start=sigma_current,
            sigma_end=target_sigma,
            model_wrapper=model_wrapper,
            extra_args=extra_args,
            max_steps=100,
            adaptive_stepping=use_adaptive_stepping
        )

        # é‡å»ºç©ºé–“è»Œè·¡
        spatial_trajectory = solution_trajectory['spatial_trajectory']

        # è¨ˆç®—æŽ§åˆ¶åºåˆ—ï¼ˆå¾žè»Œè·¡æŽ¨å°Žï¼‰
        controls = self._extract_controls_from_trajectory(
            spatial_trajectory, solution_trajectory['sigma_values'], model_wrapper, extra_args
        )

        # çµ„ç¹”è¿”å›žä¿¡æ¯
        trajectory_info = {
            'spectral_coefficients': solution_trajectory['coefficients'],
            'sigma_values': solution_trajectory['sigma_values'],
            'sobolev_norms': solve_info['sobolev_norm_history'],
            'convergence_history': solve_info['convergence_history'],
            'step_sizes': solve_info['step_size_history']
        }

        return spatial_trajectory, trajectory_info, solve_info

    def _extract_controls_from_trajectory(
        self,
        trajectory: torch.Tensor,
        sigma_values: torch.Tensor,
        model_wrapper,
        extra_args: Dict
    ) -> torch.Tensor:
        """
        å¾žæœ€å„ªè»Œè·¡ä¸­æå–æŽ§åˆ¶åºåˆ—

        Args:
            trajectory: ç©ºé–“è»Œè·¡ (T, B, C, H, W)
            sigma_values: å°æ‡‰çš„ Ïƒ å€¼
            model_wrapper: æ¨¡åž‹åŒ…è£å™¨
            extra_args: é¡å¤–åƒæ•¸

        Returns:
            controls: æŽ§åˆ¶åºåˆ— (T-1, B, C, H, W)
        """
        T = trajectory.shape[0]
        controls = torch.zeros_like(trajectory[:-1])

        for t in range(T - 1):
            x_t = trajectory[t]
            x_next = trajectory[t + 1]
            sigma_t = sigma_values[t]
            sigma_next = sigma_values[t + 1]

            # è¨ˆç®—æ™‚é–“æ­¥é•·
            dt = sigma_next - sigma_t

            if abs(dt) > 1e-12:
                # æŽ§åˆ¶ u = dx/dt
                controls[t] = (x_next - x_t) / dt
            else:
                # æ¥µå°æ­¥é•·ï¼Œä½¿ç”¨æ¨¡åž‹é æ¸¬
                s_in = torch.ones(x_t.shape[0], device=x_t.device) * sigma_t
                f_denoiser = model_wrapper(x_t, s_in, **extra_args)
                controls[t] = f_denoiser / (sigma_t + 1e-8)

        return controls

    def _solve_optimal_control_step(
        self,
        x: torch.Tensor,
        sigma: float,
        sigma_next: float,
        denoiser_function: Callable,
        extra_args: Dict
    ) -> torch.Tensor:
        """
        æ±‚è§£å–®æ­¥æœ€å„ªæŽ§åˆ¶

        ä½¿ç”¨ Hamilton-Jacobi-Bellman æ–¹ç¨‹çš„ç°¡åŒ–å½¢å¼
        """
        # ç²å–åŽ»å™ªå‡½æ•¸è¼¸å‡º
        s_in = torch.ones(x.shape[0], device=x.device) * sigma
        f_denoiser = denoiser_function(x, s_in, **extra_args)

        # è¨ˆç®—ç†æƒ³æ¼‚ç§»é …
        drift_term = f_denoiser / (sigma + 1e-8)

        # æ·»åŠ æ­£å‰‡åŒ–ä¿®æ­£
        regularization_correction = self._compute_regularization_correction(x, f_denoiser, sigma)

        # æœ€å„ªæŽ§åˆ¶: u* = f/Ïƒ + ä¿®æ­£é …
        optimal_control = drift_term + regularization_correction

        return optimal_control

    def _compute_regularization_correction(
        self,
        x: torch.Tensor,
        f_denoiser: torch.Tensor,
        sigma: float
    ) -> torch.Tensor:
        """
        è¨ˆç®—æ­£å‰‡åŒ–ä¿®æ­£é …
        """
        # è¨ˆç®— f çš„æ¢¯åº¦
        grad_f = self.action_integral._approximate_gradient(f_denoiser)

        # æ­£å‰‡åŒ–ä¿®æ­£: -Î» âˆ‡(|âˆ‡f|Â²) / 2
        grad_correction = self._compute_gradient_correction(grad_f)

        # æ›²çŽ‡ä¿®æ­£
        curvature_correction = self._compute_curvature_correction(x)

        total_correction = (
            -self.action_integral.lambda_reg * grad_correction / 2 +
            -self.action_integral.curvature_penalty * curvature_correction
        )

        return total_correction

    def _compute_gradient_correction(self, grad_f: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—æ¢¯åº¦ä¿®æ­£é …
        """
        # âˆ‡(|âˆ‡f|Â²) â‰ˆ 2âˆ‡f Â· âˆ‡(âˆ‡f)
        # ç°¡åŒ–å¯¦ç¾
        return torch.zeros_like(grad_f)

    def _compute_curvature_correction(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—æ›²çŽ‡ä¿®æ­£é …
        """
        # æ‹‰æ™®æ‹‰æ–¯ç®—å­
        if x.dim() == 4:
            laplacian = F.conv2d(
                x,
                torch.tensor([[[[0, 1, 0], [1, -4, 1], [0, 1, 0]]]],
                           device=x.device, dtype=x.dtype).expand(x.shape[1], 1, -1, -1),
                padding=1,
                groups=x.shape[1]
            )
        else:
            laplacian = torch.zeros_like(x)

        return laplacian

    def evaluate_trajectory_quality(
        self,
        trajectory: torch.Tensor,
        sigma_values: torch.Tensor,
        denoiser_function: Callable,
        extra_args: Optional[Dict] = None
    ) -> Dict[str, float]:
        """
        è©•ä¼°è»Œè·¡è³ªé‡

        ä½¿ç”¨è®Šåˆ†å‹•ä½œç©åˆ†å’Œå…¶ä»–å“è³ªæŒ‡æ¨™

        Args:
            trajectory: è»Œè·¡ (T, B, C, H, W)
            sigma_values: Ïƒ èª¿åº¦
            denoiser_function: åŽ»å™ªå‡½æ•¸
            extra_args: é¡å¤–åƒæ•¸

        Returns:
            quality_metrics: å“è³ªè©•ä¼°æŒ‡æ¨™
        """
        extra_args = extra_args or {}

        # è¨ˆç®—å‹•ä½œç©åˆ†
        action_value = self.action_integral.compute_action(
            trajectory, sigma_values, denoiser_function, extra_args
        )

        # è¨ˆç®— Sobolev ç¯„æ•¸è®ŠåŒ–
        sobolev_norms = []
        for t in range(trajectory.shape[0]):
            norm = self.hilbert_space.compute_sobolev_norm(trajectory[t])
            sobolev_norms.append(norm.item())

        # è¨ˆç®— Euler-Lagrange æ®˜å·®
        residuals = []
        for t in range(1, trajectory.shape[0] - 1):
            x_prev = trajectory[t - 1]
            x_curr = trajectory[t]
            x_next = trajectory[t + 1]
            sigma_t = sigma_values[t]

            # è¨ˆç®—å°Žæ•¸
            dt_prev = (sigma_values[t] - sigma_values[t - 1]).item()
            dt_next = (sigma_values[t + 1] - sigma_values[t]).item()

            if abs(dt_prev) > 1e-12 and abs(dt_next) > 1e-12:
                x_dot = (x_next - x_prev) / (dt_next + dt_prev)
                x_ddot = (x_next - 2 * x_curr + x_prev) / (dt_next * dt_prev / 2)

                # è¨ˆç®—åŽ»å™ªè¼¸å‡º
                s_in = torch.ones(x_curr.shape[0], device=x_curr.device) * sigma_t
                f_denoiser = denoiser_function(x_curr, s_in, **extra_args)

                # EL æ®˜å·®
                residual = self.euler_lagrange_system.compute_euler_lagrange_residual(
                    x_curr, x_dot, x_ddot, f_denoiser, sigma_t.item()
                )
                residuals.append(torch.norm(residual).item())

        return {
            'action_value': action_value.item(),
            'mean_sobolev_norm': np.mean(sobolev_norms),
            'sobolev_norm_variation': np.std(sobolev_norms),
            'mean_euler_lagrange_residual': np.mean(residuals) if residuals else 0.0,
            'max_euler_lagrange_residual': np.max(residuals) if residuals else 0.0,
            'trajectory_smoothness': self._compute_trajectory_smoothness(trajectory),
            'energy_conservation': self._compute_energy_conservation(trajectory, sigma_values)
        }

    def verify_optimality_conditions(
        self,
        trajectory: torch.Tensor,
        sigma_schedule: torch.Tensor,
        denoiser_function: Callable,
        extra_args: Optional[Dict] = None,
        tolerance: float = 1e-3
    ) -> Dict[str, bool]:
        """
        é©—è­‰æœ€å„ªæ€§æ¢ä»¶

        æª¢æŸ¥è»Œè·¡æ˜¯å¦æ»¿è¶³ Euler-Lagrange æ–¹ç¨‹

        Args:
            trajectory: è»Œè·¡
            sigma_schedule: Ïƒ èª¿åº¦
            denoiser_function: åŽ»å™ªå‡½æ•¸
            extra_args: é¡å¤–åƒæ•¸
            tolerance: å®¹å¿åº¦

        Returns:
            optimality_check: æœ€å„ªæ€§æª¢æŸ¥çµæžœ
        """
        extra_args = extra_args or {}

        # è¨ˆç®—è»Œè·¡å°Žæ•¸
        x_dot = self.action_integral._compute_trajectory_derivative(trajectory, sigma_schedule)

        # è¨ˆç®—äºŒéšŽå°Žæ•¸ (ç°¡åŒ–)
        x_ddot = torch.zeros_like(x_dot)
        for t in range(1, len(x_dot) - 1):
            dt1 = sigma_schedule[t] - sigma_schedule[t-1]
            dt2 = sigma_schedule[t+1] - sigma_schedule[t]
            if abs(dt1) > 1e-12 and abs(dt2) > 1e-12:
                x_ddot[t] = (x_dot[t+1] - x_dot[t-1]) / (dt1 + dt2)

        euler_lagrange_satisfied = True
        max_residual = 0.0

        for t in range(1, len(trajectory) - 1):
            x_t = trajectory[t]
            x_dot_t = x_dot[t]
            x_ddot_t = x_ddot[t]
            sigma_t = sigma_schedule[t]

            # ç²å–åŽ»å™ªå‡½æ•¸è¼¸å‡º
            s_in = torch.ones(x_t.shape[0], device=x_t.device) * sigma_t
            f_t = denoiser_function(x_t, s_in, **extra_args)

            # è¨ˆç®— Euler-Lagrange æ®˜å·®
            residual = self.euler_lagrange_system.compute_euler_lagrange_residual(
                x_t, x_dot_t, x_ddot_t, f_t, sigma_t.item()
            )

            residual_norm = torch.norm(residual).item()
            max_residual = max(max_residual, residual_norm)

            if residual_norm > tolerance:
                euler_lagrange_satisfied = False

        return {
            'euler_lagrange_satisfied': euler_lagrange_satisfied,
            'max_residual_norm': max_residual,
            'tolerance': tolerance,
            'residual_within_tolerance': max_residual <= tolerance
        }