# VariationalController æ¨¡å¡Šèªªæ˜æ–‡æª”

## æ¦‚è¿°

`VariationalController` æ˜¯ ISDO ç³»çµ±çš„æ ¸å¿ƒæ§åˆ¶å™¨ï¼Œå¯¦ç¾äº†è®Šåˆ†æœ€å„ªæ§åˆ¶ç†è«–ï¼Œå°‡å‚³çµ±çš„æ“´æ•£æ¡æ¨£å•é¡Œè½‰åŒ–ç‚ºåœ¨å¸Œçˆ¾ä¼¯ç‰¹ç©ºé–“ä¸­æ±‚è§£æœ€å„ªè·¯å¾‘çš„è®Šåˆ†å•é¡Œã€‚é€™æ˜¯ ISDO ç®—æ³•è¶…è¶Šå‚³çµ± ODE æ±‚è§£å™¨çš„é—œéµæ‰€åœ¨ã€‚

## æ•¸å­¸ç†è«–åŸºç¤

### è®Šåˆ†æœ€å„ªæ§åˆ¶å•é¡Œ

ISDO å°‡æ“´æ•£æ¡æ¨£é‡æ–°å»ºæ¨¡ç‚ºæœ€å„ªæ§åˆ¶å•é¡Œï¼š

**ç›®æ¨™**: å°‹æ‰¾å¾å™ªè² x(Ïƒ_max) åˆ°æ•¸æ“š x(0) çš„æœ€å„ªè»Œè·¡ï¼Œæœ€å°åŒ–å‹•ä½œç©åˆ†ï¼š

```
ğ’œ[x] = âˆ«[Ïƒ_max to 0] â„’(x, áº‹, Ïƒ) dÏƒ
```

å…¶ä¸­æ‹‰æ ¼æœ—æ—¥å‡½æ•¸ç‚ºï¼š
```
â„’(x, áº‹, Ïƒ) = Â½|áº‹ - f(x;Ïƒ)/Ïƒ|Â²_H + Î»|âˆ‡_x f|Â²_op + Î¼|âˆ‡Â²x|Â²
```

### Euler-Lagrange æ–¹ç¨‹

æœ€å„ªè»Œè·¡å¿…é ˆæ»¿è¶³ Euler-Lagrange æ–¹ç¨‹ï¼š

```
d/dÏƒ(âˆ‚â„’/âˆ‚áº‹) - âˆ‚â„’/âˆ‚x = 0
```

é€™çµ¦å‡ºäº†æ¯”å‚³çµ± ODE æ›´ç²¾ç¢ºçš„å‹•åŠ›å­¸æ–¹ç¨‹ã€‚

### Hamilton-Jacobi-Bellman æ–¹ç¨‹

å°æ–¼å€¼å‡½æ•¸ V(x, Ïƒ)ï¼ŒHJB æ–¹ç¨‹ç‚ºï¼š

```
âˆ‚V/âˆ‚Ïƒ + H(x, âˆ‡V, Ïƒ) = 0
```

å…¶ä¸­ Hamiltonian ç‚ºï¼š
```
H(x, p, Ïƒ) = min_u [âŸ¨p, uâŸ© + â„’(x, u, Ïƒ)]
```

## æ ¸å¿ƒåŠŸèƒ½

### 1. å‹•ä½œç©åˆ†è¨ˆç®—

```python
from modules_forge.isdo.core.variational_controller import VariationalController

# åˆå§‹åŒ–è®Šåˆ†æ§åˆ¶å™¨
controller = VariationalController(
    spatial_dims=(64, 64),
    regularization_lambda=0.01,  # Î»: æ­£å‰‡åŒ–å¼·åº¦
    curvature_penalty=0.001,     # Î¼: æ›²ç‡æ‡²ç½°
    sobolev_order=1.5,          # Sobolev ç©ºé–“éšæ•¸
    device=torch.device('cuda')
)

# è¨ˆç®—æœ€å„ªæ§åˆ¶è»Œè·¡
x_init = torch.randn(1, 3, 64, 64)  # åˆå§‹å™ªè²
sigma_start = 10.0
sigma_end = 0.1

optimal_trajectory, optimal_controls = controller.compute_optimal_control(
    x_current=x_init,
    sigma_current=sigma_start,
    denoiser_function=model,  # å»å™ªæ¨¡å‹
    target_sigma=sigma_end,
    num_steps=50,
    extra_args={'conditioning': conditioning_info}
)

print(f"è»Œè·¡å½¢ç‹€: {optimal_trajectory.shape}")  # (51, 1, 3, 64, 64)
print(f"æ§åˆ¶å½¢ç‹€: {optimal_controls.shape}")   # (50, 1, 3, 64, 64)
```

### 2. æ‹‰æ ¼æœ—æ—¥å‡½æ•¸è¨ˆç®—

å–®æ­¥æ‹‰æ ¼æœ—æ—¥å‡½æ•¸è¨ˆç®—ï¼š

```python
# è¨ˆç®—å–®é»çš„æ‹‰æ ¼æœ—æ—¥å‡½æ•¸å€¼
x = torch.randn(1, 3, 64, 64)
x_dot = torch.randn(1, 3, 64, 64)  # dx/dÏƒ
f_denoiser = model(x, sigma * torch.ones(1))  # å»å™ªå‡½æ•¸è¼¸å‡º
sigma = 5.0

lagrangian_value = controller.action_integral.compute_lagrangian(
    x=x,
    x_dot=x_dot,
    f_denoiser=f_denoiser,
    sigma=sigma
)

print(f"æ‹‰æ ¼æœ—æ—¥å‡½æ•¸å€¼: {lagrangian_value.item():.6f}")
```

### 3. å®Œæ•´è»Œè·¡å‹•ä½œè¨ˆç®—

```python
# è©•ä¼°æ•´å€‹è»Œè·¡çš„å‹•ä½œç©åˆ†
sigma_schedule = torch.linspace(10.0, 0.1, 51)

def denoiser_wrapper(x, s_in, **kwargs):
    return model(x, s_in)

action_value = controller.action_integral.compute_action(
    trajectory=optimal_trajectory,
    sigma_schedule=sigma_schedule,
    denoiser_function=denoiser_wrapper,
    extra_args={}
)

print(f"ç¸½å‹•ä½œå€¼: {action_value.item():.3f}")
```

## é«˜ç´šåŠŸèƒ½

### 1. è»Œè·¡è³ªé‡è©•ä¼°

å…¨é¢è©•ä¼°è»Œè·¡çš„å„é …è³ªé‡æŒ‡æ¨™ï¼š

```python
quality_metrics = controller.evaluate_trajectory_quality(
    trajectory=optimal_trajectory,
    sigma_schedule=sigma_schedule,
    denoiser_function=denoiser_wrapper
)

print("è»Œè·¡è³ªé‡è©•ä¼°:")
for metric, value in quality_metrics.items():
    print(f"  {metric}: {value:.6f}")
```

è¼¸å‡ºç¤ºä¾‹ï¼š
```
è»Œè·¡è³ªé‡è©•ä¼°:
  action_value: 15.234567
  initial_sobolev_norm: 12.345678
  final_sobolev_norm: 3.456789
  trajectory_smoothness: 2.345678
  energy_change: -8.888999
  norm_ratio: 0.280124
```

### 2. æœ€å„ªæ€§æ¢ä»¶é©—è­‰

é©—è­‰è»Œè·¡æ˜¯å¦æ»¿è¶³ Euler-Lagrange æ–¹ç¨‹ï¼š

```python
optimality_check = controller.verify_optimality_conditions(
    trajectory=optimal_trajectory,
    sigma_schedule=sigma_schedule,
    denoiser_function=denoiser_wrapper,
    tolerance=1e-3
)

print("æœ€å„ªæ€§æª¢æŸ¥:")
print(f"  Euler-Lagrange æ»¿è¶³: {optimality_check['euler_lagrange_satisfied']}")
print(f"  æœ€å¤§æ®˜å·®ç¯„æ•¸: {optimality_check['max_residual_norm']:.2e}")
print(f"  åœ¨å®¹å¿ç¯„åœå…§: {optimality_check['residual_within_tolerance']}")
```

### 3. è‡ªé©æ‡‰æ§åˆ¶ç­–ç•¥

å¯¦ç¾è‡ªé©æ‡‰çš„è®Šåˆ†æ§åˆ¶ï¼š

```python
def adaptive_variational_sampling(
    model,
    x_init,
    sigma_max=10.0,
    sigma_min=0.01,
    quality_threshold=0.01
):
    """
    è‡ªé©æ‡‰è®Šåˆ†æ¡æ¨£ï¼Œæ ¹æ“šè»Œè·¡è³ªé‡å‹•æ…‹èª¿æ•´æ­¥æ•¸
    """
    current_x = x_init
    current_sigma = sigma_max
    trajectory_points = [current_x]

    while current_sigma > sigma_min:
        # ä¼°ç®—ä¸‹ä¸€æ­¥çš„ç›®æ¨™ sigma
        sigma_step = min(current_sigma * 0.8, current_sigma - sigma_min)
        target_sigma = current_sigma - sigma_step

        # è¨ˆç®—æœ€å„ªæ§åˆ¶
        traj, controls = controller.compute_optimal_control(
            x_current=current_x,
            sigma_current=current_sigma,
            denoiser_function=lambda x, s: model(x, s),
            target_sigma=target_sigma,
            num_steps=10
        )

        # è©•ä¼°è³ªé‡
        sigma_schedule = torch.linspace(current_sigma, target_sigma, 11)
        quality = controller.evaluate_trajectory_quality(
            trajectory=traj,
            sigma_schedule=sigma_schedule,
            denoiser_function=lambda x, s: model(x, s)
        )

        # æ ¹æ“šè³ªé‡èª¿æ•´
        if quality['trajectory_smoothness'] > quality_threshold:
            # è³ªé‡ä¸ä½³ï¼Œæ¸›å°æ­¥é•·
            sigma_step *= 0.5
            target_sigma = current_sigma - sigma_step
            continue

        # æ¥å—é€™ä¸€æ­¥
        current_x = traj[-1]
        current_sigma = target_sigma
        trajectory_points.append(current_x)

        print(f"Ïƒ: {current_sigma:.3f}, å‹•ä½œå€¼: {quality['action_value']:.3f}")

    return torch.stack(trajectory_points, dim=0)

# ä½¿ç”¨è‡ªé©æ‡‰æ¡æ¨£
adaptive_result = adaptive_variational_sampling(model, x_init)
```

## å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹

### 1. è¶…é«˜è§£æåº¦åœ–åƒç”Ÿæˆ

è®Šåˆ†æ§åˆ¶ç‰¹åˆ¥é©åˆé«˜è§£æåº¦åœ–åƒç”Ÿæˆï¼š

```python
def high_resolution_generation(model, resolution=1024):
    """
    é«˜è§£æåº¦åœ–åƒçš„è®Šåˆ†æ¡æ¨£
    """
    # å‰µå»ºé«˜è§£æåº¦æ§åˆ¶å™¨
    hr_controller = VariationalController(
        spatial_dims=(resolution, resolution),
        regularization_lambda=0.005,  # é™ä½æ­£å‰‡åŒ–é¿å…éåº¦å¹³æ»‘
        curvature_penalty=0.0001,     # è¼•å¾®æ›²ç‡ç´„æŸ
        sobolev_order=1.2,           # è¼ƒä½éšæ•¸é©åˆé«˜é »ç´°ç¯€
    )

    # åˆå§‹åŒ–å™ªè²
    x_init = torch.randn(1, 3, resolution, resolution)

    # ä½¿ç”¨æ›´ç²¾ç´°çš„ Ïƒ èª¿åº¦
    sigma_schedule = torch.logspace(1, -2, 100)  # 100 æ­¥ç²¾ç´°èª¿åº¦

    # åˆ†æ®µå„ªåŒ–
    trajectory_segments = []
    current_x = x_init

    for i in range(0, len(sigma_schedule)-10, 10):
        segment_sigmas = sigma_schedule[i:i+11]

        segment_traj, _ = hr_controller.compute_optimal_control(
            x_current=current_x,
            sigma_current=segment_sigmas[0].item(),
            denoiser_function=lambda x, s: model(x, s),
            target_sigma=segment_sigmas[-1].item(),
            num_steps=10
        )

        trajectory_segments.append(segment_traj)
        current_x = segment_traj[-1]

        # ç›£æ§è³ªé‡
        quality = hr_controller.evaluate_trajectory_quality(
            trajectory=segment_traj,
            sigma_schedule=segment_sigmas,
            denoiser_function=lambda x, s: model(x, s)
        )
        print(f"æ®µ {i//10}: å‹•ä½œå€¼ {quality['action_value']:.3f}")

    return torch.cat(trajectory_segments, dim=0)

# ç”Ÿæˆé«˜è§£æåº¦åœ–åƒ
hr_result = high_resolution_generation(model, resolution=512)
final_image = hr_result[-1]  # æœ€çµ‚çµæœ
```

### 2. æ¢ä»¶ç”Ÿæˆçš„ç²¾ç¢ºæ§åˆ¶

åˆ©ç”¨è®Šåˆ†æ§åˆ¶å¯¦ç¾ç²¾ç¢ºçš„æ¢ä»¶ç”Ÿæˆï¼š

```python
def conditional_variational_generation(
    model,
    conditioning,
    control_strength=1.0
):
    """
    æ¢ä»¶ç”Ÿæˆçš„è®Šåˆ†æ§åˆ¶
    """
    # ä¿®æ”¹æ‹‰æ ¼æœ—æ—¥å‡½æ•¸ä»¥åŒ…å«æ¢ä»¶ç´„æŸ
    class ConditionalActionIntegral(controller.action_integral.__class__):
        def __init__(self, parent, conditioning, strength):
            super().__init__(
                parent.lambda_reg,
                parent.curvature_penalty,
                parent.domain_size
            )
            self.conditioning = conditioning
            self.strength = strength

        def compute_lagrangian(self, x, x_dot, f_denoiser, sigma, grad_f=None):
            # åŸºç¤æ‹‰æ ¼æœ—æ—¥é …
            base_lagrangian = super().compute_lagrangian(
                x, x_dot, f_denoiser, sigma, grad_f
            )

            # æ¢ä»¶ç´„æŸé …
            if self.conditioning is not None:
                condition_error = torch.sum((x - self.conditioning)**2, dim=(-2, -1))
                condition_penalty = self.strength * condition_error
                base_lagrangian = base_lagrangian + condition_penalty

            return base_lagrangian

    # å‰µå»ºæ¢ä»¶æ§åˆ¶å™¨
    conditional_controller = VariationalController(
        spatial_dims=(64, 64),
        regularization_lambda=0.01,
        curvature_penalty=0.001,
        sobolev_order=1.5
    )

    # æ›¿æ›å‹•ä½œç©åˆ†è¨ˆç®—å™¨
    conditional_controller.action_integral = ConditionalActionIntegral(
        conditional_controller.action_integral,
        conditioning,
        control_strength
    )

    # åŸ·è¡Œæ¢ä»¶ç”Ÿæˆ
    x_init = torch.randn(1, 3, 64, 64)

    trajectory, controls = conditional_controller.compute_optimal_control(
        x_current=x_init,
        sigma_current=10.0,
        denoiser_function=lambda x, s: model(x, s, conditioning=conditioning),
        target_sigma=0.1,
        num_steps=50
    )

    return trajectory[-1]

# ä½¿ç”¨æ¢ä»¶ç”Ÿæˆ
conditioning_info = torch.randn(1, 3, 64, 64)  # æ¢ä»¶ä¿¡æ¯
conditional_result = conditional_variational_generation(
    model, conditioning_info, control_strength=0.5
)
```

### 3. å¤šæ¨¡æ…‹æ¡æ¨£ç­–ç•¥

åˆ©ç”¨è®Šåˆ†æ§åˆ¶æ¢ç´¢å¤šæ¨¡æ…‹åˆ†ä½ˆï¼š

```python
def multimodal_variational_sampling(model, num_modes=4):
    """
    å¤šæ¨¡æ…‹è®Šåˆ†æ¡æ¨£
    """
    results = []

    for mode in range(num_modes):
        # æ¯å€‹æ¨¡æ…‹ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–å’Œåƒæ•¸
        x_init = torch.randn(1, 3, 64, 64) * (1.0 + 0.2 * mode)

        # å‹•æ…‹èª¿æ•´æ­£å‰‡åŒ–å¼·åº¦
        mode_controller = VariationalController(
            spatial_dims=(64, 64),
            regularization_lambda=0.005 * (1 + mode * 0.5),  # éå¢æ­£å‰‡åŒ–
            curvature_penalty=0.001 / (1 + mode * 0.2),      # éæ¸›æ›²ç‡ç´„æŸ
            sobolev_order=1.5
        )

        trajectory, _ = mode_controller.compute_optimal_control(
            x_current=x_init,
            sigma_current=10.0,
            denoiser_function=lambda x, s: model(x, s),
            target_sigma=0.1,
            num_steps=50
        )

        # è©•ä¼°æ¨¡æ…‹è³ªé‡
        quality = mode_controller.evaluate_trajectory_quality(
            trajectory=trajectory,
            sigma_schedule=torch.linspace(10.0, 0.1, 51),
            denoiser_function=lambda x, s: model(x, s)
        )

        results.append({
            'sample': trajectory[-1],
            'quality': quality,
            'mode_id': mode
        })

        print(f"æ¨¡æ…‹ {mode}: å‹•ä½œå€¼ {quality['action_value']:.3f}")

    # é¸æ“‡æœ€ä½³æ¨¡æ…‹æˆ–è¿”å›æ‰€æœ‰çµæœ
    best_mode = min(results, key=lambda x: x['quality']['action_value'])

    return {
        'best_sample': best_mode['sample'],
        'all_samples': [r['sample'] for r in results],
        'qualities': [r['quality'] for r in results]
    }

# å¤šæ¨¡æ…‹æ¡æ¨£
multimodal_results = multimodal_variational_sampling(model, num_modes=3)
```

## æ€§èƒ½å„ªåŒ–

### 1. ä¸¦è¡ŒåŒ–è»Œè·¡è¨ˆç®—

```python
def parallel_trajectory_computation(controller, x_batch, model):
    """
    æ‰¹æ¬¡ä¸¦è¡Œè¨ˆç®—å¤šå€‹è»Œè·¡
    """
    batch_size = x_batch.shape[0]

    # ä¸¦è¡Œè¨ˆç®—æ‰€æœ‰è»Œè·¡
    trajectories = []

    for i in range(batch_size):
        traj, _ = controller.compute_optimal_control(
            x_current=x_batch[i:i+1],
            sigma_current=10.0,
            denoiser_function=lambda x, s: model(x, s),
            target_sigma=0.1,
            num_steps=50
        )
        trajectories.append(traj)

    # åˆä½µçµæœ
    batched_trajectories = torch.stack(trajectories, dim=1)  # (T, B, C, H, W)

    return batched_trajectories

# æ‰¹æ¬¡è™•ç†
x_batch = torch.randn(4, 3, 64, 64)  # 4 å€‹æ¨£æœ¬
batch_results = parallel_trajectory_computation(controller, x_batch, model)
```

### 2. è¨˜æ†¶é«”å„ªåŒ–çš„é•·è»Œè·¡

```python
def memory_efficient_long_trajectory(controller, x_init, model, total_steps=1000):
    """
    è¨˜æ†¶é«”å„ªåŒ–çš„é•·è»Œè·¡è¨ˆç®—
    """
    checkpoint_interval = 50
    current_x = x_init
    current_sigma = 10.0
    sigma_min = 0.01

    trajectory_checkpoints = [current_x]

    num_checkpoints = total_steps // checkpoint_interval
    sigma_schedule = torch.logspace(1, -2, num_checkpoints + 1)

    for i in range(num_checkpoints):
        target_sigma = sigma_schedule[i + 1].item()

        # è¨ˆç®—æ®µè»Œè·¡
        segment_traj, _ = controller.compute_optimal_control(
            x_current=current_x,
            sigma_current=current_sigma,
            denoiser_function=lambda x, s: model(x, s),
            target_sigma=target_sigma,
            num_steps=checkpoint_interval
        )

        # åªä¿å­˜æª¢æŸ¥é»ï¼Œé‡‹æ”¾ä¸­é–“çµæœ
        current_x = segment_traj[-1].clone()
        current_sigma = target_sigma
        trajectory_checkpoints.append(current_x)

        # å¼·åˆ¶åƒåœ¾å›æ”¶
        del segment_traj
        torch.cuda.empty_cache()

        print(f"æª¢æŸ¥é» {i+1}/{num_checkpoints}, Ïƒ: {current_sigma:.4f}")

    return torch.stack(trajectory_checkpoints, dim=0)
```

## èª¿è©¦èˆ‡è¨ºæ–·

### 1. æ”¶æ–‚æ€§åˆ†æ

```python
def analyze_convergence(controller, trajectory, sigma_schedule, model):
    """
    åˆ†æè»Œè·¡çš„æ”¶æ–‚æ€§è³ª
    """
    # è¨ˆç®—æ¯æ­¥çš„å‹•ä½œè®ŠåŒ–
    action_history = []

    for t in range(len(trajectory) - 1):
        segment = trajectory[t:t+2]
        segment_sigmas = sigma_schedule[t:t+2]

        action_val = controller.action_integral.compute_action(
            segment, segment_sigmas, lambda x, s: model(x, s)
        )
        action_history.append(action_val.item())

    # åˆ†ææ”¶æ–‚ç‡
    action_diffs = np.diff(action_history)
    convergence_rate = np.mean(np.abs(action_diffs))

    # æª¢æŸ¥å–®èª¿æ€§
    is_monotonic = np.all(action_diffs <= 0)  # å‹•ä½œæ‡‰è©²éæ¸›

    print(f"å¹³å‡æ”¶æ–‚ç‡: {convergence_rate:.2e}")
    print(f"å‹•ä½œå–®èª¿éæ¸›: {is_monotonic}")

    return {
        'action_history': action_history,
        'convergence_rate': convergence_rate,
        'monotonic': is_monotonic
    }

# åˆ†ææ”¶æ–‚æ€§
convergence_info = analyze_convergence(
    controller, optimal_trajectory, sigma_schedule, model
)
```

### 2. æ•¸å€¼ç©©å®šæ€§æª¢æŸ¥

```python
def check_numerical_stability(controller, trajectory):
    """
    æª¢æŸ¥æ•¸å€¼è¨ˆç®—çš„ç©©å®šæ€§
    """
    # æª¢æŸ¥è»Œè·¡ä¸­çš„ç•°å¸¸å€¼
    trajectory_norms = torch.norm(trajectory.view(len(trajectory), -1), dim=1)

    # æª¢æ¸¬çˆ†ç‚¸æˆ–æ¶ˆå¤±
    max_norm = torch.max(trajectory_norms)
    min_norm = torch.min(trajectory_norms)
    condition_number = max_norm / (min_norm + 1e-12)

    # æª¢æŸ¥æ¢¯åº¦
    x_dot = controller.action_integral._compute_trajectory_derivative(
        trajectory, torch.linspace(10.0, 0.1, len(trajectory))
    )
    gradient_norms = torch.norm(x_dot.view(len(x_dot), -1), dim=1)
    max_gradient = torch.max(gradient_norms)

    stability_ok = (
        condition_number < 1e6 and
        max_norm < 1e3 and
        max_gradient < 1e3
    )

    return {
        'stable': stability_ok,
        'condition_number': condition_number.item(),
        'max_norm': max_norm.item(),
        'max_gradient': max_gradient.item()
    }
```

## ç–‘é›£æ’è§£

### å¸¸è¦‹å•é¡ŒåŠè§£æ±ºæ–¹æ¡ˆ

1. **å‹•ä½œç©åˆ†çˆ†ç‚¸**
   ```python
   # é™ä½æ­£å‰‡åŒ–åƒæ•¸
   controller = VariationalController(
       spatial_dims=(64, 64),
       regularization_lambda=0.001,  # å¾ 0.01 é™åˆ° 0.001
       curvature_penalty=0.0001,
       sobolev_order=1.0  # é™ä½ Sobolev éšæ•¸
   )
   ```

2. **è»Œè·¡ä¸å¹³æ»‘**
   ```python
   # å¢åŠ æ›²ç‡æ‡²ç½°
   controller.action_integral.curvature_penalty = 0.01  # å¢åŠ åˆ° 0.01

   # æˆ–ä½¿ç”¨æ›´å¤šç©åˆ†æ­¥æ•¸
   trajectory, _ = controller.compute_optimal_control(
       x_current=x_init,
       sigma_current=10.0,
       denoiser_function=model,
       target_sigma=0.1,
       num_steps=100  # å¢åŠ æ­¥æ•¸
   )
   ```

3. **è¨˜æ†¶é«”ä¸è¶³**
   ```python
   # ä½¿ç”¨æ¼¸é€²å¼è¨ˆç®—
   def progressive_computation(controller, x_init, model):
       current_x = x_init
       sigma_points = torch.logspace(1, -2, 21)  # 20 æ®µ

       for i in range(len(sigma_points) - 1):
           traj, _ = controller.compute_optimal_control(
               x_current=current_x,
               sigma_current=sigma_points[i].item(),
               denoiser_function=model,
               target_sigma=sigma_points[i+1].item(),
               num_steps=5  # çŸ­æ®µè¨ˆç®—
           )
           current_x = traj[-1]
           torch.cuda.empty_cache()

       return current_x
   ```

## åƒè€ƒæ–‡ç»

- Bertsekas, D.P. "Dynamic Programming and Optimal Control"
- Pontryagin, L.S. "Mathematical Theory of Optimal Processes"
- Fleming, W.H. "Controlled Markov Processes and Viscosity Solutions"
- Evans, L.C. "An Introduction to Mathematical Optimal Control Theory"

---

**æ³¨æ„**: VariationalController æ¶‰åŠè¤‡é›œçš„è®Šåˆ†è¨ˆç®—ï¼Œå»ºè­°å…ˆç†è§£ç¶“å…¸è®Šåˆ†æ³•å’Œæœ€å„ªæ§åˆ¶ç†è«–ã€‚å¯¦éš›ä½¿ç”¨æ™‚ï¼Œåƒæ•¸èª¿å„ªå°çµæœå½±éŸ¿å¾ˆå¤§ã€‚